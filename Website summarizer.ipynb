{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fbf7fc-5ea3-472d-8c99-141653005ea5",
   "metadata": {},
   "source": [
    "# Project: Automatic Web Summarizer and Brochure Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17123cd-28d2-4e35-8dae-d1cba4169015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "An Automatic Web Summarizer and Brochure Generator.\n",
    "\n",
    "This script uses AI models to either:\n",
    "  1) Summarize the content of a given website.\n",
    "  2) Identify relevant links on the website for brochure generation and create a short company overview.\n",
    "\n",
    "The workflow:\n",
    "  - Prompt the user to select an AI model (either 'gpt-4o-mini' or 'llama3.2').\n",
    "  - Prompt the user to choose a feature: summarize or create brochure content.\n",
    "  - Prompt the user to enter a valid, reachable URL.\n",
    "  - Based on the user’s choices, fetch and parse the page(s), then provide summarizations or compile a brochure in Markdown format.\n",
    "\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "#----------------------------------IMPORTS------------------------------------#\n",
    "###############################################################################\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "###############################################################################\n",
    "#------------------------------GLOBAL VARIABLES-------------------------------#\n",
    "###############################################################################\n",
    "\n",
    "# A list of AI models that this script can interface with.\n",
    "available_models = [\n",
    "    \"gpt-4o-mini\", \n",
    "    \"llama3.2\"\n",
    "]\n",
    "\n",
    "# Dictionary mapping integer string keys to feature descriptions.\n",
    "available_features = {\n",
    "    \"1\": \"Web Summarizer\",\n",
    "    \"2\": \"Brochure Generator\"\n",
    "}\n",
    "\n",
    "# Define a set of headers to replicate a typical browser request (often necessary to access certain websites).\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/110.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "#-----------------------------------CLASSES-----------------------------------#\n",
    "###############################################################################\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    The Website class encapsulates methods for fetching a webpage, removing irrelevant content,\n",
    "    and retrieving its basic textual data, including the title and body. \n",
    "    It also extracts all hyperlinks for further processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Constructor that initializes and fetches a website’s contents.\n",
    "        \n",
    "        :param url: The URL of the webpage to fetch.\n",
    "        \n",
    "        Steps:\n",
    "            1) Send an HTTP GET request to the provided URL using a predefined 'headers' dictionary.\n",
    "            2) Parse the HTML using BeautifulSoup.\n",
    "            3) Extract the page title if it exists; otherwise, store a default placeholder.\n",
    "            4) Remove script, style, img, and input tags to avoid clutter.\n",
    "            5) Collect textual data by joining the remaining HTML elements with line breaks.\n",
    "            6) Extract all hyperlinks found on the page and store them in 'self.links'.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        \n",
    "        # Send an HTTP GET request to fetch the page content.\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Create a BeautifulSoup object to parse and traverse the HTML DOM.\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Retrieve page title if present; store a fallback string if absent.\n",
    "        if soup.title:\n",
    "            self.title = soup.title.string\n",
    "        else:\n",
    "            self.title = \"No title found\"\n",
    "        \n",
    "        # Check if the webpage actually has a body. If so, remove irrelevant tags.\n",
    "        if soup.body:\n",
    "            # Remove scripts, styles, images, and form inputs, which usually do not contain textual content relevant for summarization.\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            # After removing these tags, extract remaining text, preserving paragraph breaks using '\\n'.\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            # If there's no body tag, store an empty string to avoid errors in subsequent usage.\n",
    "            self.text = \"\"\n",
    "\n",
    "        # Gather all hyperlinks from <a> tags within the parsed document.\n",
    "        links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "        # Filter out None or empty links and store them in self.links.\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        \"\"\"\n",
    "        Presents the webpage’s title and main body text in a simple, readable format.\n",
    "        \n",
    "        :return: A string containing the webpage title followed by the parsed body text.\n",
    "        \"\"\"\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n",
    "\n",
    "###############################################################################\n",
    "#----------------------------------FUNCTIONS----------------------------------#\n",
    "###############################################################################\n",
    "\n",
    "def chat_with_model(api_params):\n",
    "    \"\"\"\n",
    "    Interacts with the chosen AI model (OpenAI or Ollama) by passing the necessary parameters.\n",
    "    \n",
    "    :param api_params: A dictionary containing the prompts and other metadata required by the model.\n",
    "    :return: The AI's response (plain text) as a string.\n",
    "    \n",
    "    This function switches between:\n",
    "        1) 'gpt-4o-mini' using the OpenAI client API.\n",
    "        2) 'llama3.2' using Ollama's chat interface.\n",
    "    \"\"\"\n",
    "    # 'user_model' is set globally in this script (assigned in the main user-interaction section).\n",
    "    if user_model == \"gpt-4o-mini\":\n",
    "        # Load environment variables from a .env file for the OpenAI API key.\n",
    "        load_dotenv(override=True)\n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        # Raise an error if the API key isn't available. This enforces a strict requirement to set up .env.\n",
    "        if not api_key:\n",
    "            raise ValueError(\"❌ OPENAI_API_KEY is not set in the environment.\")\n",
    "        \n",
    "        # Create an instance of the OpenAI class with the retrieved API key.\n",
    "        openai_client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        # The 'chat.completions.create()' method is used to generate a completion from a chat-based model.\n",
    "        response = openai_client.chat.completions.create(**api_params)\n",
    "        \n",
    "        # Return the text content from the first choice in the AI model’s response.\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    elif user_model == \"llama3.2\":\n",
    "        # Ollama does not support 'response_format'; remove if present to avoid possible errors.\n",
    "        if \"response_format\" in api_params:\n",
    "            api_params.pop(\"response_format\")\n",
    "        \n",
    "        # Pass the parameters to Ollama's 'chat' function. The returned object contains multiple keys; \n",
    "        # we focus on 'message' -> 'content' for the textual response.\n",
    "        response = ollama.chat(**api_params)\n",
    "        return response[\"message\"][\"content\"]\n",
    "    \n",
    "    else:\n",
    "        # If an invalid model is selected, raise an error with the list of permissible models.\n",
    "        raise ValueError(f\"❌ Invalid model '{user_model}'. Choose from: {available_models}\")\n",
    "\n",
    "\n",
    "def stream_output():\n",
    "    \"\"\"\n",
    "    Streams the response from the selected AI model incrementally,\n",
    "    updating the Markdown display in a Jupyter-like environment.\n",
    "    \n",
    "    This approach allows partial updates of the UI, rather than \n",
    "    waiting for the entire response to finish before display.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    # 'display' is imported from IPython.display to facilitate interactive output updates.\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    # For each 'chunk' in the model's response, append it to the existing response string, \n",
    "    # then display the updated text in Markdown format (cleaning up triple backticks in the process).\n",
    "    for chunk in chat_with_model(api_params):\n",
    "        response += chunk or ''\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "\n",
    "def is_valid_url(url):\n",
    "    \"\"\"\n",
    "    Checks if the provided URL string is valid according to a regular expression pattern.\n",
    "    \n",
    "    :param url: The URL string to test.\n",
    "    :return: True if the URL is syntactically valid, otherwise False.\n",
    "    \"\"\"\n",
    "    # Regex pattern checks for optional http/https, then an optional 'www.', followed by \n",
    "    # domain name(s), and optional path segments.\n",
    "    pattern = re.compile(r\"^(https?://)?(www\\.)?[a-zA-Z-]+(\\.[a-zA-Z]{2,})+(/.*)?$\")\n",
    "    return bool(pattern.match(url))\n",
    "\n",
    "\n",
    "def is_reachable_url(url):\n",
    "    \"\"\"\n",
    "    Determines if the specified URL can be reached by sending an HTTP GET request.\n",
    "    \n",
    "    :param url: The URL string to validate for reachability.\n",
    "    :return: True if the URL returns a 200 OK status code, otherwise False.\n",
    "    \n",
    "    Additional notes:\n",
    "      - If the URL doesn't begin with 'http://' or 'https://', \n",
    "        the function prepends 'https://' by default.\n",
    "      - The 'requests.get()' call includes a timeout to prevent indefinite blocking.\n",
    "      - A RequestException is caught for generic issues such as timeouts or refused connections.\n",
    "    \"\"\"\n",
    "    # Ensure the URL starts with a valid protocol scheme; otherwise, default to 'https://'.\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        url = \"https://\" + url\n",
    "\n",
    "    try:\n",
    "        # Attempt to fetch the URL with a specified timeout and allow_redirects.\n",
    "        response = requests.get(url, headers=headers, timeout=5, allow_redirects=True)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        # Return False if any request error is raised (timeout, connection error, etc.).\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_all_details(url):\n",
    "    \"\"\"\n",
    "    Gathers textual data from the main page (landing page) and from each link \n",
    "    found relevant for the brochure feature. \n",
    "    \n",
    "    :param url: The primary website URL selected by the user.\n",
    "    :return: A string containing the textual contents of the main page \n",
    "             and any additional links deemed relevant for brochure creation.\n",
    "    \n",
    "    Steps:\n",
    "        1) Fetch and parse the main landing page of the URL provided.\n",
    "        2) Iterate over 'selected_links[\"links\"]', which is expected to be \n",
    "           a JSON with objects of the form: {\"type\": \"...\", \"url\": \"...\"}.\n",
    "        3) Concatenate each page's content to a single consolidated string.\n",
    "    \"\"\"\n",
    "    # Initialize our result string with a header identifying the landing page.\n",
    "    result = \"Landing page:\\n\"\n",
    "    \n",
    "    # Use the Website class to retrieve the main page’s content.\n",
    "    result += Website(url).get_contents()\n",
    "    \n",
    "    # For each relevant link, add a small title (the \"type\" field) and fetch its content.\n",
    "    for link in selected_links[\"links\"]:\n",
    "        # Denote the type of page to provide context.\n",
    "        result += f\"\\n\\n{link['type']}\\n\"\n",
    "        result += Website(link[\"url\"]).get_contents()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#----------------------------------MAIN CODE----------------------------------#\n",
    "###############################################################################\n",
    "\n",
    "# Display a friendly welcome message and prompt the user to select an AI model.\n",
    "print(\n",
    "    \"🚀 WELCOME to the Automatic Web Summarizer & Brochure Generator!\\n\"\n",
    "    \"Let's start by selecting the AI model you would like to use.\\n\"\n",
    "    f\"Available models: {', '.join(available_models)}\"\n",
    ")\n",
    "\n",
    "# Loop until the user enters a valid model name.\n",
    "while True:\n",
    "    # Request the user’s model choice and convert to lowercase for consistent comparison.\n",
    "    user_model = input(\"👉 Enter your choice: \").lower().strip()\n",
    "    \n",
    "    if user_model in available_models:\n",
    "        print(f\"\\n✅ {user_model} selected.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\n❌ Invalid model. Please choose from: {', '.join(available_models)}\\n\")\n",
    "\n",
    "# Ask the user to choose a feature: summarization (1) or brochure generation (2).\n",
    "print(\n",
    "    \"\\n🎯 Now select the feature you would like to use:\\n\"\n",
    "    \"1️⃣ Web Summarizer\\n\"\n",
    "    \"2️⃣ Brochure Generator\"\n",
    ")\n",
    "\n",
    "# Loop until a valid feature choice (1 or 2) is entered.\n",
    "while True:\n",
    "    feature_selection = input(\"👉 Enter your choice (1 or 2): \").strip()\n",
    "    \n",
    "    if feature_selection in available_features:\n",
    "        print(f\"\\n✅ {available_features[feature_selection]} selected.\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"\\n❌ Invalid choice. Please enter '1' for Web Summarizer or '2' for Brochure Generator.\\n\")\n",
    "\n",
    "# Continuously prompt for a URL and validate it until a valid, reachable URL is entered.\n",
    "while True:\n",
    "    url = input(\"\\n🌐 Enter a website URL: \")\n",
    "\n",
    "    # 1) Check if the URL's syntax is correct.\n",
    "    if not is_valid_url(url):\n",
    "        print(\"\\n❌ Invalid format! Make sure to enter a proper URL (e.g., https://example.com).\\n\")\n",
    "    # 2) Check if the URL returns a 200 status code.\n",
    "    elif not is_reachable_url(url):\n",
    "        print(\"\\n⚠️ The website appears to be unreachable. Please try another URL.\\n\")\n",
    "    else:\n",
    "        # If the protocol scheme is missing, default to 'https://'.\n",
    "        if not url.startswith((\"http://\", \"https://\")):\n",
    "            url = \"https://\" + url\n",
    "        \n",
    "        print(f\"\\n✅ The website '{url}' is valid and reachable. 🚀\")\n",
    "        break\n",
    "\n",
    "# Instantiate the Website class for the main landing page.\n",
    "website = Website(url)\n",
    "\n",
    "# ------------------------- FEATURE 1: WEB SUMMARIZER -------------------------\n",
    "if feature_selection == \"1\":\n",
    "    # The system prompt sets the AI's role and guides it to produce a structured summary.\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that analyzes the contents of a website \"\n",
    "        \"and provides a detailed summary, ignoring text that might be navigation-related. \"\n",
    "        \"Respond in Markdown format.\"\n",
    "    )\n",
    "\n",
    "    # The user prompt contains the raw content to be summarized, plus instructions on the desired format.\n",
    "    user_prompt = (\n",
    "        f\"You are analyzing a website titled: **{website.title}**\\n\\n\"\n",
    "        \"### 📌 Website Content Overview:\\n\"\n",
    "        f\"{website.text}\\n\\n\"\n",
    "        \"🔍 **Task:**\\n\"\n",
    "        \"- Summarize the website content in Markdown format.\\n\"\n",
    "        \"- If the website contains **news or announcements**, provide a summary of those as well.\"\n",
    "    )\n",
    "\n",
    "    # The 'messages' structure aligns with OpenAI’s Chat Completion API requirements.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # Define parameters for the AI model, including which model is used and the prompt messages.\n",
    "    api_params = {\n",
    "        \"model\": user_model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    # Print status message and invoke the 'stream_output' function to display results incrementally.\n",
    "    print(f\"\\n📝 Generating a summary for the website: **{website.title}**.\\n\")\n",
    "    stream_output()\n",
    "\n",
    "# ------------------------- FEATURE 2: BROCHURE GENERATOR -------------------------\n",
    "elif feature_selection == \"2\":\n",
    "    # For the brochure generation feature, we must first retrieve and parse relevant links from the webpage.\n",
    "    link_system_prompt = (\n",
    "        \"You are given a list of links extracted from a company's webpage. \"\n",
    "        \"Your task is to determine which links are most relevant for inclusion in a company brochure.\\n\\n\"\n",
    "        \"⚠️ IMPORTANT: Respond with a valid JSON object ONLY. No extra text, no explanations, no formatting issues.\\n\\n\"\n",
    "        \"Your response MUST follow this JSON structure:\\n\"\n",
    "        \"```json\\n\"\n",
    "        \"{\\n\"\n",
    "        '    \"links\": [\\n'\n",
    "        '        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\\n'\n",
    "        '        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\\n'\n",
    "        \"    ]\\n\"\n",
    "        \"}\\n\"\n",
    "        \"```\"\n",
    "    )\n",
    "\n",
    "    # Build the prompt for the AI by listing the discovered links on the main page.\n",
    "    link_user_prompt = (\n",
    "        f\"Here is the list of links found on the website of {website.url}.\\n\\n\"\n",
    "        \"🔍 **Task:** Identify and return only the relevant links suitable for a company brochure. \"\n",
    "        \"Ensure that the response includes the **full https URL** in JSON format.\\n\\n\"\n",
    "        \"**🚫 Do NOT include:**\\n\"\n",
    "        \"- Terms of Service\\n\"\n",
    "        \"- Privacy Policy\\n\"\n",
    "        \"- Email links\\n\\n\"\n",
    "        \"**🔗 Links (some might be relative):**\\n\"\n",
    "    )\n",
    "    link_user_prompt += \"\\n\".join(website.links)\n",
    "\n",
    "    # Compile messages for the AI. The system prompt clarifies the response requirements,\n",
    "    # and the user prompt provides actual data (the link list).\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": link_user_prompt}\n",
    "    ]\n",
    "\n",
    "    # Parameters for the AI call, specifying a JSON response format for link analysis.\n",
    "    api_params = {\n",
    "        \"model\": user_model,\n",
    "        \"messages\": messages,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "\n",
    "    print(\"\\nObtaining relevant links...\\n\")\n",
    "    # Make a direct call (without streaming) to retrieve the JSON data from the model.\n",
    "    selected_links = json.loads(chat_with_model(api_params))\n",
    "\n",
    "    # Once we have the relevant links, we parse those pages and gather their textual content.\n",
    "    print(\"Gathering information from relevant links...\\n\")\n",
    "\n",
    "    # The next system prompt explains how to build a brochure from these selected links.\n",
    "    system_prompt = (\n",
    "        \"You are an assistant that analyzes the contents of several relevant pages from a company website \"\n",
    "        \"and creates a short brochure about the company for prospective customers, investors, and recruits. \"\n",
    "        \"Respond in Markdown format. Include details of company culture, customers, and careers/jobs if available.\"\n",
    "    )\n",
    "\n",
    "    # Compose a user prompt that includes the landing page content plus each relevant link.\n",
    "    user_prompt = (\n",
    "        f\"You are looking at a company called: {website.title}\\n\"\n",
    "        \"Here are the contents of its landing page and other relevant pages; \"\n",
    "        \"use this information to build a short brochure of the company in Markdown.\\n\"\n",
    "    )\n",
    "    user_prompt += get_all_details(url)\n",
    "\n",
    "    # Prepare the final messages for the AI to produce the brochure.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    api_params = {\n",
    "        \"model\": user_model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    print(f\"\\n📝 Generating a brochure for the company: **{website.title}**.\\n\")\n",
    "    # Stream the AI's output for a real-time user experience.\n",
    "    stream_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe79c9-0147-47c9-8cd4-f7f7b5dfc098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
